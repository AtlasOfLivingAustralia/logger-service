version: 0.2
###
# This build project exports any variables needed for later stages and actions,
# builds the template configs used to launch the CloudFormation templates

env:
  shell: bash
  git-credential-helper: yes
  variables:
    DEBIAN_FRONTEND: "noninteractive"
  exported-variables:
    - BASE_STACK_FILE_PFIX
    - BASE_STACK_NAME
    - CODEBUILD_BUILD_NUMBER
    - CODEBUILD_BUILD_NUMBER
    - COGNITO_STACK_NAME
    - DB_READ_ENDPOINT
    - DB_WRITE_ENDPOINT
    - DOMAIN_NAME
    - EKS_CLUSTER_NAME
    - HELM_RELEASE_NAME
    - CERTIFICATE_ARN
    - HOSTED_ZONE
    - PRODUCT_COMPONENT
    - PRODUCT_NAME
    - SECRET_NAME
    - EKS_NAMESPACE
    - ACCESS_LOGS_S3_BUCKET_NAME
    - SLACK_ALERT_CHANNEL
    - SLACK_DEPLOY_NOTIFICATION

phases:

  install:
    commands:
      - echo Running on $(lsb_release -d | cut -f2)
      - echo aws-cli version $(aws --version)
      - pip install jinja2
      - export CUR_PIPELINE_FINGERPRINT=$(md5sum cicd/$PRODUCT_COMPONENT/pipeline/pipeline.yaml | awk '{print $1}')
      - # This next bit checks if the running pipeline is out of sync with the pipeline in the
      - # current code revision. If it is it re-launches itself! For a normal branch commit the
      - # pipeline is set to autorun on update so it will restart automatically and launch the
      - # latest revision on the branch. For a rollback we dont want it to restart automatically as
      - # we need to run a specific commit, not the latest. In this case the pipeline is set NOT to
      - # autorun on update. It will have to be manually started after the rollback pipeline
      - # has finished launching
      - | 
          if [[ $PIPELINE_FINGERPRINT != $CUR_PIPELINE_FINGERPRINT ]]; then
            echo existing pipeline is out of sync with current code revision, relaunching!
            cd cicd/$PRODUCT_COMPONENT/pipeline/
            ./deploy_pipeline.sh -e ${ENVIRONMENT:0:4} -b $SRC_BRANCH
            # pipeline execution should now stop if this is a rollback, 
            # or restart automatically if this a normal branch deploy
            exit 1
          else
            echo existing pipeline is in sync with current code revision, proceeding with the deploy!
          fi
    finally:
      - #echo This always runs even if the update or install command fails

  pre_build:
    commands:
      - echo Entered the pre_build phase...
      - echo source branch is $SRC_BRANCH
      - echo clean branch is $CLEAN_BRANCH
      - echo Environment is $ENVIRONMENT
      - echo generating environment vars...
      - cicd/gen_env_vars.py --env $ENVIRONMENT --clean-branch $CLEAN_BRANCH --conf cicd/$PRODUCT_COMPONENT/config.ini > env.txt
      - echo loading config..
      - set -a ; source env.txt ; set +a
      # import stack export from the base cloudformation stack
      - echo importing stack output...
      - DB_READ_ENDPOINT=$(aws cloudformation describe-stacks --stack-name $DATABASE_STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='ReadEndpoint'].OutputValue" --output text)
      - DB_WRITE_ENDPOINT=$(aws cloudformation describe-stacks --stack-name $DATABASE_STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='WriteEndpoint'].OutputValue" --output text)
      - ACCESS_LOGS_S3_BUCKET_NAME=$(aws cloudformation describe-stacks --stack-name $BASE_STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='LoadBalancerAccessLogsBucket'].OutputValue" --output text)
      - SECRET_NAME=$(aws cloudformation describe-stacks --stack-name $BASE_STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='LoggerSecret'].OutputValue" --output text)
      - echo DB_READ_ENDPOINT=$DB_READ_ENDPOINT
      - echo DB_WRITE_ENDPOINT=$DB_WRITE_ENDPOINT
      - echo ACCESS_LOGS_S3_BUCKET_NAME=$ACCESS_LOGS_S3_BUCKET_NAME
      - echo SECRET_NAME=$SECRET_NAME
      - echo BASE_STACK_NAME=$BASE_STACK_NAME
      - export EKS_CLUSTER_NAME=$(aws cloudformation list-exports --query "Exports[?Name=='$REGOLITH_STACK_NAME-ClusterName'].Value" --output text)
      - echo EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME
      - DB_PASSWORD=$(aws secretsmanager get-secret-value --secret-id "$SECRET_NAME" --query 'SecretString' --output text | jq -r '.["db-password"]')
      - CAS_CLIENT_SECRET=$(aws secretsmanager get-secret-value --secret-id "$SECRET_NAME" --query 'SecretString' --output text | jq -r '.["cas-client-secret"]')
    finally:
      - #echo This always runs

  build:
    commands:
      - echo Entered the build phase...
      - DB_NAME=$PRODUCT_NAME$ENVIRONMENT
      - DB_HOSTNAME=$([[ "$ENVIRONMENT" == "development" ]] && echo "$CLEAN_BRANCH" || echo "$ENVIRONMENT")
      - |
        USER_POOL_ID=$(aws cloudformation describe-stacks --stack-name $COGNITO_STACK_NAME \
          --query "Stacks[0].Outputs[?OutputKey=='UserPoolId'].OutputValue" --output text)
      - |
        CLIENT_ID=$(aws cloudformation describe-stacks --stack-name $BASE_STACK_NAME \
          --query "Stacks[0].Outputs[?OutputKey=='LoggerAppClient'].OutputValue" --output text)
      - |
        CLIENT_SECRET=$(aws cognito-idp describe-user-pool-client --user-pool-id $USER_POOL_ID \
          --client-id $CLIENT_ID --query "UserPoolClient.ClientSecret" --output text)
      - |
        if [[ "$ENVIRONMENT" == "production" ]]; then
          CLIENT_ID=${CAS_CLIENT_ID}
          CLIENT_SECRET=${CAS_CLIENT_SECRET}
        fi
      - |
        echo "
        logger_base_url: "https://${DOMAIN_NAME}"
        logger_context_path: "${CONTEXT_PATH}"
        cas:
          server_name: "${CAS_AUTH_BASE_URL}"
        db:
          database_name: "${DB_NAME}"
          username: "${DB_USERNAME}"
          password: "${DB_PASSWORD}"
        ala:
          baseURL: "${ALA_BASE_URL}"
        bie:
          baseURL: "${BIE_BASE_URL}"
        collections:
          baseURL: "${COLLECTORY_URL}"
        headerAndFooter:
          baseURL: "${HEADER_AND_FOOTER_BASEURL}"
          version: "${HEADER_AND_FOOTER_VERSION}"
        oidc:
          clientId: "${CLIENT_ID}"
          secret: "${CLIENT_SECRET}"
          discoveryUri: "${OIDC_DISCOVERY_URI}"
          logoutUrl: "${OIDC_LOGOUT_URL}"
          alaUseridClaim: "${OIDC_ALA_USERID_CLAIM}"
          logout_action: "${OIDC_LOGOUT_ACTION}"
          scope: "${OIDC_SCOPE}"
        core:
          roleAttribute: "${CORE_ROLE_ATTRIBUTE}"
        cookie:
          auth_cookie_enabled: "${COOKIE_AUTH_COOKIE_ENABLED}"
          auth_cookie_domain: "${COOKIE_AUTH_COOKIE_DOMAIN}"
        jwt:
          rolesFromAccessToken: "${JWT_ROLES_FROM_ACCESS_TOKEN}"
          userIdClaim: "${JWT_USER_ID_CLAIM}"
          roleClaims: "${JWT_ROLE_CLAIMS}"
        apikey:
          apikey_check_enabled: "${APIKEY_CHECK_ENABLED}"
        webservice:
          webservice_jwt: "${WEBSERVICE_JWT}"
        userdetails_api_url: "${USERDETAILS_API_URL}"
        userdetails_url: "${USERDETAILS_URL}"
        oauth2:
          baseUrl: "${OPENAPI_OAUTH_URL}"
        podEnvironment: "${POD_ENVIRONMENT}"
        xmx: "${HEAP_SIZE_INITIAL}"
        xms: "${HEAP_SIZE_MAX}"
        ingress:
          accessLogsPrefix: "logs"
          accessLogsBucket: "${ACCESS_LOGS_S3_BUCKET_NAME}"
        resources:
          requests:
            cpu: "${CPU_REQUEST}"      
            memory: "${MEMORY_REQUEST}" 
          limits:
            cpu: "${CPU_LIMIT}"      
            memory: "${MEMORY_LIMIT}"" > helm-values.yaml

    finally:
      - #echo This always runs


  post_build:
    commands:
      - #echo Entered the post_build phase...

artifacts:
  files:
    - '**/*'
